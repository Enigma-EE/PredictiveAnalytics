---
title: |
  | INFS_SP5_2023
  | Predictive Analytics
  | Assignment 3 
  | Classification

author: 
  - "Huining Huang: huahy057@mymail.unisa.edu.au" 
output: 
  pdf_document:
    toc: true
editor_options:
  chunk_output_type: console
---



```{r echo = FALSE, include=FALSE}
# clear all variables, functions, etc
# clean up memory
rm(list=ls())
# clean up memory
gc()
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  fig.width = 8, 
  fig.asp = 0.618, 
  out.width = "80%",
  fig.align = "center", 
  root.dir = "../",
  message = FALSE,
  size = "small"
)
```


```{r warning=FALSE, include=FALSE}
pacman::p_load(tidyverse)
pacman::p_load(knitr,dplyr,AICcmodavg)
pacman::p_load(inspectdf,tidyr,stringr, stringi,DT,mice)
pacman::p_load(caret,modelr)
pacman::p_load(mlbench,mplot)
pacman::p_load(tidymodels,glmx)
pacman::p_load(skimr,vip,yardstick,ranger,kknn,funModeling,Hmisc)
pacman::p_load(ggplot2,ggpubr,GGally)
knitr::opts_chunk$set(message = FALSE)
```



```{r warning=FALSE, include=FALSE}
# Decision tree
pacman::p_load(rpart.plot)

# Data manipulation
pacman::p_load(rgl, rattle, mice, dplyr, tidyverse)

# Plotting
pacman::p_load(viridis, hrbrthemes, ggplot2, heplots, ggpubr, forcats)
```

```{r warning=FALSE, include=FALSE}
# Load data
data <- read.csv("data.csv", header = TRUE, sep = ",")
nrow(data)
names(data)
# head(data)
```




**Data Splitting**


```{r warning=FALSE, include=FALSE}
set.seed(1000)  # Setting a seed for reproducibility

total_rows <- nrow(data)
train_size <- 0.8 * total_rows
validation_size <- 0.1 * total_rows
test_size <- 0.1 * total_rows

# Splitting the data into train, validation, and test sets
train_index <- sample(1:total_rows, train_size)
remaining_data <- setdiff(1:total_rows, train_index)
validation_index <- sample(remaining_data, validation_size)
test_index <- setdiff(remaining_data, validation_index)

train <- data[train_index, ]
validation <- data[validation_index, ]
test <- data[test_index, ]
```


```{r warning=FALSE, include=FALSE}
# Move PotentialFraud to the last column in the train dataset
train <- train[c(setdiff(names(train), "PotentialFraud"), "PotentialFraud")]

# Move PotentialFraud to the last column in the test dataset
test <- test[c(setdiff(names(test), "PotentialFraud"), "PotentialFraud")]

validation <- validation[c(setdiff(names(validation), "PotentialFraud"), "PotentialFraud")]
```


```{r warning=FALSE, include=FALSE}
head(train)
```


Pivot Study Application: Finally, employing ensemble methods like Gradient Boosting or AdaBoost allows to potentially improve upon the performance of the individual models previously built. In a pivot study, you would use insights gained from the simpler models to fine-tune these more complex, powerful methods. This is where your initial XGBoost model fits in, as it's a type of gradient boosting.

```{r warning=FALSE, include=FALSE}
library(xgboost)
library(caret)

# Ensure that PotentialFraud is the last column
dtrain <- xgb.DMatrix(data = as.matrix(train[,-ncol(train)]), label = train$PotentialFraud)

# Simplified parameter grid
param_grid <- expand.grid(
  max_depth = c(10, 30),  # Fewer values for max_depth
  min_child_weight = c(1, 3),  # Fewer values for min_child_weight
  eta = c(0.01, 0.1),  # Fewer values for learning rate
  nrounds = 50  # Reduced number of boosting rounds
)

# Reduced number of folds for cross-validation
cv.nfold <- 3

# Grid search
best_model <- NULL
best_performance <- Inf

for(i in 1:nrow(param_grid)) {
  params <- list(
    objective = "binary:logistic",
    max_depth = param_grid$max_depth[i],
    min_child_weight = param_grid$min_child_weight[i],
    eta = param_grid$eta[i]
  )
  
  model <- xgb.train(params = params, data = dtrain, nrounds = param_grid$nrounds[i])
  
  # Evaluate the model using cross-validation
  cv_results <- xgb.cv(params = params, data = dtrain, nrounds = param_grid$nrounds[i], nfold = cv.nfold, metrics = "auc")
  mean_auc <- mean(cv_results$evaluation_log$test_auc_mean)
  
  # If the model performance is better, update best_model and best_performance
  if(mean_auc < best_performance) {
    best_performance <- mean_auc
    best_model <- model
  }
}

# Check the best model
best_model

```



```{r warning=FALSE, include=FALSE}
# save the best model
saveRDS(best_model, "best_model_em.rds")
```



---



### 1. Evaluate Model Performance
To evaluate your `xgb.Booster` model, you'll need to make predictions on a validation or test dataset and then calculate the relevant metrics. Assuming you have a separate test dataset, you can proceed as follows:

```{r warning=FALSE, include=FALSE}
# Make predictions
dtest <- xgb.DMatrix(data = as.matrix(test[,-ncol(test)]), label = test$PotentialFraud)
preds <- predict(best_model, dtest)
pred_labels <- ifelse(preds > 0.5, 1, 0) # Assuming 0.5 as the threshold

# Calculate metrics
conf_matrix <- table(Predicted = pred_labels, Actual = test$PotentialFraud)
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
precision <- conf_matrix[2,2] / sum(conf_matrix[2,])
recall <- conf_matrix[2,2] / sum(conf_matrix[,2])
f1_score <- 2 * precision * recall / (precision + recall)

# For AUC
library(pROC)
roc_obj <- roc(response = test$PotentialFraud, predictor = preds)
auc_value <- auc(roc_obj)
```


> best_model
##### xgb.Booster
raw: 1.6 Mb
call:
  xgb.train(params = params, data = dtrain, nrounds = param_grid$nrounds[i])
params (as set within xgb.train):
  objective = "binary:logistic", max_depth = "10", min_child_weight = "3", eta = "0.01", validate_parameters = "TRUE"
xgb.attributes:
  niter
callbacks:
  cb.print.evaluation(period = print_every_n)
# of features: 35
niter: 50
nfeatures : 35

```{r warning=FALSE, include=FALSE, echo=FALSE}
# Print the metrics
print("Confusion Matrix:")
print(conf_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
print(paste("Precision:", round(precision, 4)))
print(paste("Recall:", round(recall, 4)))
print(paste("F1-Score:", round(f1_score, 4)))
print(paste("AUC:", round(auc_value, 4)))
```


> # Print the metrics
> print("Confusion Matrix:")
[1] "Confusion Matrix:"
> print(conf_matrix)
         Actual
Predicted     0     1
        0 30429 11749
        1  4056  9588
> print(paste("Accuracy:", round(accuracy, 4)))
[1] "Accuracy: 0.7169"
> print(paste("Precision:", round(precision, 4)))
[1] "Precision: 0.7027"
> print(paste("Recall:", round(recall, 4)))
[1] "Recall: 0.4494"
> print(paste("F1-Score:", round(f1_score, 4)))
[1] "F1-Score: 0.5482"
> print(paste("AUC:", round(auc_value, 4)))
[1] "AUC: 0.7591"

### 2. Interpret the Results
- **Model Parameters**: The printed result shows that the best model has `max_depth = 10`, `min_child_weight = 1`, and `eta = 0.01`. These parameters indicate how the model was structured: a maximum depth of 10 layers, a minimum child weight (similar to `minsplit` in decision trees) of 1, and a learning rate of 0.01.
- **Number of Iterations and Features**: The model ran for 100 iterations (boosting rounds) and used 35 features for the prediction.

### 3. Comparing with the Baseline Model
- **Metrics Comparison**: Compare the calculated accuracy, precision, recall, F1-score, and AUC of your `xgboost` model with those of the baseline model. Higher values generally indicate better performance. Pay special attention to AUC, as it's a robust metric for binary classification tasks.
- **Parameter Impact**: The selection of `max_depth`, `min_child_weight`, and `eta` in the `xgboost` model directly impacts its ability to learn from the training data. The `max_depth` controls the complexity of the model, `min_child_weight` helps in preventing overfitting by controlling the minimum sum of weights needed in a child, and `eta` is the learning rate that controls the model's sensitivity to new patterns in data.

### 4. Plotting the Decision Tree
`xgboost` models are ensemble models, meaning they combine the predictions from many trees. It's not straightforward to visualize these models as a single decision tree. However, you can visualize individual trees or feature importance:

```{r warning=FALSE, include=FALSE}
# load the DiagrammeR package
pacman::p_load(DiagrammeR)

xgb.plot.tree(model = best_model, trees = 0)

```


```{r warning=FALSE, include=FALSE}
# Calculate feature importance
importance_matrix <- xgb.importance(feature_names = colnames(train[,-ncol(train)]), model = best_model)

# Plot feature importance for top 10 features
xgb.plot.importance(importance_matrix, top_n = 10)

```

```{r warning=FALSE, include=FALSE}
importance_matrix
```

> importance_matrix
                            Feature         Gain        Cover    Frequency
 1:                           State 4.024179e-01 4.215594e-01 2.176262e-01
 2:                          County 3.757779e-01 2.408960e-01 3.751477e-01
 3:            Log_TotalClaimAmount 6.798028e-02 1.031512e-01 5.364680e-02
 4:                       PHY330576 4.495202e-02 6.794506e-02 3.840959e-03
 5:                       PHY412132 3.022318e-02 3.195556e-02 5.487084e-03
 6:            IsSamePhysMultiRole1 2.960324e-02 5.530552e-02 1.924700e-02
 7:                 UniquePhysCount 2.900586e-02 5.747800e-02 1.836063e-02
 8:                       PHY337425 9.307702e-03 6.718488e-03 3.883167e-03
 9:               Log_OPTotalAmount 2.525183e-03 2.924590e-03 7.850751e-02
10:                             Age 2.071707e-03 2.227413e-03 6.234172e-02
11:               Log_IPTotalAmount 1.743061e-03 3.721669e-03 4.385447e-02
12:            IsSamePhysMultiRole2 5.082021e-04 2.331070e-03 3.967584e-03
13:      ChronicCond_ObstrPulmonary 4.416912e-04 1.740651e-04 1.004559e-02
14:           ChronicCond_Alzheimer 3.714002e-04 1.222083e-04 9.370252e-03
15: ChronicCond_rheumatoidarthritis 3.436678e-04 1.991689e-04 8.821543e-03
16:        ChronicCond_Heartfailure 3.074134e-04 1.124272e-04 9.074793e-03
17:           RenalDiseaseIndicator 2.447979e-04 7.244999e-05 5.613709e-03
18:              ChronicCond_Cancer 2.280382e-04 2.006060e-04 6.922168e-03
19:           ClmAdmitDiagnosisCode 2.184588e-04 4.340623e-04 4.811751e-03
20:                          Gender 2.151847e-04 1.491183e-04 9.117002e-03
21:            ChronicCond_Diabetes 2.127811e-04 6.966096e-05 5.993584e-03
22:              ClmDiagnosisCode_9 2.070760e-04 7.264497e-04 6.289043e-03
23:          ChronicCond_Depression 1.958667e-04 9.611924e-05 8.821543e-03
24:                            Race 1.754443e-04 5.772202e-04 6.668918e-03
25:        ChronicCond_Osteoporasis 1.633356e-04 1.430573e-04 6.204626e-03
26:                WeekendAdmission 1.449651e-04 2.822155e-04 2.405875e-03
27:       ChronicCond_KidneyDisease 9.695167e-05 5.400246e-05 3.207834e-03
28:        ClaimSettlementDelay_Cat 7.905993e-05 6.546181e-05 2.743542e-03
29:       ChronicCond_IschemicHeart 6.100256e-05 2.485943e-05 1.941584e-03
30:              ClmDiagnosisCode_1 5.553006e-05 7.966056e-05 8.019585e-04
31:              ChronicCond_stroke 5.412798e-05 1.235418e-04 2.659125e-03
32:              ClmDiagnosisCode_2 3.906360e-05 3.025384e-05 1.857167e-03
33:             ClmDiagnosisCode_10 2.192400e-05 8.093956e-06 6.331251e-04
34:           TreatmentDuration_Cat 5.031012e-06 2.986100e-05 4.220834e-05
35:                          IsDead 9.561381e-07 1.135544e-05 4.220834e-05
                  
      Importance
 1: 4.024179e-01
 2: 3.757779e-01
 3: 6.798028e-02
 4: 4.495202e-02
 5: 3.022318e-02
 6: 2.960324e-02
 7: 2.900586e-02
 8: 9.307702e-03
 9: 2.525183e-03
10: 2.071707e-03
11: 1.743061e-03
12: 5.082021e-04
13: 4.416912e-04
14: 3.714002e-04
15: 3.436678e-04
16: 3.074134e-04
17: 2.447979e-04
18: 2.280382e-04
19: 2.184588e-04
20: 2.151847e-04
21: 2.127811e-04
22: 2.070760e-04
23: 1.958667e-04
24: 1.754443e-04
25: 1.633356e-04
26: 1.449651e-04
27: 9.695167e-05
28: 7.905993e-05
29: 6.100256e-05
30: 5.553006e-05
31: 5.412798e-05
32: 3.906360e-05
33: 2.192400e-05
34: 5.031012e-06
35: 9.561381e-07

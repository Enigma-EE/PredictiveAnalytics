{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From C:\\Users\\E\\AppData\\Local\\Temp\\ipykernel_9916\\2900391042.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is GPU available: True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices visible to TensorFlow\n",
    "# This will provide a list of all devices TensorFlow can access\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# Specifically check for a GPU\n",
    "# This will return a list of GPU devices\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Check if a GPU is being used\n",
    "# This will return True if TensorFlow is able to access the GPU\n",
    "print(\"Is GPU available:\", tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# check the tensorfow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "for column in data.columns:\n",
    "    plt.figure()\n",
    "    data[column].hist(bins=50)\n",
    "    plt.title(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ClmAdmitDiagnosisCode', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
      "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'Gender', 'Race',\n",
      "       'RenalDiseaseIndicator', 'State', 'County', 'ChronicCond_Alzheimer',\n",
      "       'ChronicCond_Heartfailure', 'ChronicCond_KidneyDisease',\n",
      "       'ChronicCond_Cancer', 'ChronicCond_ObstrPulmonary',\n",
      "       'ChronicCond_Depression', 'ChronicCond_Diabetes',\n",
      "       'ChronicCond_IschemicHeart', 'ChronicCond_Osteoporasis',\n",
      "       'ChronicCond_rheumatoidarthritis', 'ChronicCond_stroke',\n",
      "       'PotentialFraud', 'Age', 'WeekendAdmission', 'IsDead',\n",
      "       'ClaimSettlementDelay_Cat', 'TreatmentDuration_Cat',\n",
      "       'Log_TotalClaimAmount', 'Log_IPTotalAmount', 'Log_OPTotalAmount',\n",
      "       'UniquePhysCount', 'IsSamePhysMultiRole1', 'IsSamePhysMultiRole2',\n",
      "       'PHY412132', 'PHY337425', 'PHY330576'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# view the columns\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to drop due to low variance: {'ClmDiagnosisCode_10', 'PHY412132', 'PHY330576', 'PHY337425', 'IsDead'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Features and labels separation\n",
    "# Assuming 'target' is the column name for your target variable\n",
    "X = data.drop('PotentialFraud', axis=1)\n",
    "y = data['PotentialFraud']\n",
    "\n",
    "# Fit the VarianceThreshold transformer to identify low-variance features\n",
    "threshold = 0.01  # This is an example threshold, adjust it according to your needs\n",
    "sel = VarianceThreshold(threshold=threshold)\n",
    "sel.fit(X)\n",
    "\n",
    "# Get features that meet the variance threshold\n",
    "features_to_keep = X.columns[sel.get_support(indices=True)]\n",
    "\n",
    "# If any features have low variance, print them\n",
    "features_to_drop = set(X.columns) - set(features_to_keep)\n",
    "if features_to_drop:\n",
    "    print(f\"Features to drop due to low variance: {features_to_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with low variance\n",
    "data.drop(features_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping features with low variance is a technique used in feature selection to improve the performance of a model. The rationale behind this approach is based on the assumption that features with low variance carry less informative power to differentiate between the classes or outputs in your data. Here's a breakdown of why features with low variance might be considered for removal:\n",
    "\n",
    "1. **Lack of Discriminative Power**: A feature with low variance means that it has very similar values for most of the samples. If a feature is almost constant, it provides little information to the model because it does not vary much between different instances. For instance, if a feature takes the same value in 99% of the samples, it's unlikely to help the model distinguish between different classes.\n",
    "\n",
    "2. **Noise Reduction**: Features with very low variance might be mostly noise, and by removing them, you can reduce the noise level in your dataset, allowing the model to focus on more significant features.\n",
    "\n",
    "3. **Computational Efficiency**: Removing features with low variance can reduce the dimensionality of the dataset, leading to faster training times and potentially less complex models. This is especially beneficial when dealing with large datasets.\n",
    "\n",
    "4. **Overfitting Prevention**: High-dimensional data with many features relative to the number of samples can lead to overfitting, where the model learns the noise in the training data instead of the actual patterns. Reducing the number of features can help mitigate this risk.\n",
    "\n",
    "5. **Model Interpretability**: Models with fewer features are generally easier to interpret and understand. By eliminating low-variance features, you can simplify the model, making it easier to analyze and explain.\n",
    "\n",
    "However, it's important to apply this technique judiciously:\n",
    "\n",
    "- **Context Matters**: Always consider the specific context and domain knowledge. Some low-variance features might still be important for prediction if they have a significant impact on the output variable in specific cases.\n",
    "- **Threshold Selection**: The choice of the variance threshold is subjective and should be informed by domain knowledge and exploratory data analysis. A threshold that's too high might eliminate important features, while a threshold that's too low might retain too much noise.\n",
    "- **Complementary Techniques**: Feature selection based on variance should be complemented with other feature selection techniques, such as univariate statistical tests, recursive feature elimination, or feature importance from models.\n",
    "\n",
    "In summary, while dropping low-variance features can be beneficial in many scenarios, it's not a one-size-fits-all solution and should be considered as part of a broader strategy of feature engineering and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target to binary and split data\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=123)\n",
    "\n",
    "# Separate features and target variable\n",
    "X_train = train_data.drop('PotentialFraud', axis=1)\n",
    "y_train = train_data['PotentialFraud']\n",
    "X_test = test_data.drop('PotentialFraud', axis=1)\n",
    "y_test = test_data['PotentialFraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import HyperModel\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hp.Int('units_input', 64, 256, step=32), \n",
    "                        activation='relu', \n",
    "                        input_shape=self.input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(hp.Float('dropout_input', 0.0, 0.5)))\n",
    "\n",
    "        for i in range(hp.Int('n_layers', 1, 3)):\n",
    "            model.add(Dense(units=hp.Int(f'units_{i}', 32, 128, step=32),\n",
    "                            activation=hp.Choice(f'activation_{i}', ['relu', 'elu', 'selu']),\n",
    "                            kernel_regularizer=regularizers.l2(hp.Float(f'regularizer_{i}', 1e-5, 1e-3, sampling='log'))))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Dropout(hp.Float(f'dropout_{i}', 0.0, 0.5)))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "# Instantiate the hypermodel\n",
    "hypermodel = MyHyperModel(input_shape=(X_train.shape[1],))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning with Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir\\keras_tuner_demo\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Initialize the Random Search tuner\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='keras_tuner_demo'\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuner.search(X_train, y_train, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 10 Complete [00h 14m 38s]\n",
    "\n",
    "val_accuracy: 0.6420661807060242\n",
    "\n",
    "Best val_accuracy So Far: 0.6638377010822296\n",
    "\n",
    "Total elapsed time: 01h 52m 42s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and Fit the Model with the Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Compile the best model with a simpler optimizer if needed\n",
    "best_model.compile(\n",
    "    optimizer='sgd',  # Simpler optimizer\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define the early stopping callback with increased patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Fit the best model with early stopping, smaller number of epochs, and potentially smaller batch size\n",
    "best_model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10,  # Fewer epochs\n",
    "    batch_size=32,  # Smaller batch size\n",
    "    validation_split=0.1,  # Smaller validation split\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the best model\n",
    "eval_result = best_model.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the model with the best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(\n",
    "    optimizer=Adam(learning_rate=best_hps.get('learning_rate')),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define the early stopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Fit the best model with early stopping\n",
    "best_model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=50, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the best model\n",
    "eval_result = best_model.evaluate(X_test, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

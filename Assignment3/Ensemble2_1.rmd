


```{r warning=FALSE, include=FALSE}
pacman::p_load(tidyverse)
pacman::p_load(knitr,dplyr,AICcmodavg)
pacman::p_load(inspectdf,tidyr,stringr, stringi,DT,mice)
pacman::p_load(caret,modelr)
pacman::p_load(mlbench,mplot)
pacman::p_load(tidymodels,glmx)
pacman::p_load(skimr,vip,yardstick,ranger,kknn,funModeling,Hmisc)
pacman::p_load(ggplot2,ggpubr,GGally)
knitr::opts_chunk$set(message = FALSE)
```



```{r warning=FALSE, include=FALSE}
# Decision tree
pacman::p_load(rpart.plot)

# Data manipulation
pacman::p_load(rgl, rattle, mice, dplyr, tidyverse)

# Plotting
pacman::p_load(viridis, hrbrthemes, ggplot2, heplots, ggpubr, forcats)

```


```{r warning=FALSE, include=FALSE}
pacman::p_load(xgboost)

# Load from CSV files
train <- read.csv("train_important.csv")
validation <- read.csv("validation_important.csv")
test <- read.csv("test_important.csv")
```

```{r warning=FALSE, include=FALSE}

library(randomForest)

model1 <- readRDS("best_model_em.rds")
model2 <- readRDS("best_model_em2.rds")


dnew <- xgb.DMatrix(data = as.matrix(test[,-ncol(test)]))

# Generate predictions from the base models on the new data
base_model_preds_new1 <- predict(model1, dnew)
base_model_preds_new2 <- predict(model2, dnew)

# Combine the base model predictions with the new data
stacked_predictions_new <- data.frame(
  model1_preds = base_model_preds_new1,
  model2_preds = base_model_preds_new2,
  PotentialFraud = test$PotentialFraud  # Include the target variable for the new data
)

# Train the Random Forest as the Meta-Model
meta_model_rf <- randomForest(PotentialFraud ~ ., data = stacked_predictions_new, family = binomial())


# Predict using the Random Forest Meta-Model
# meta_preds_rf_new <- predict(meta_model_rf, newdata = stacked_predictions_new, type = "response")

```


```{r warning=FALSE, include=FALSE}
library(pROC)

# Make predictions with the stacked ensemble model
meta_preds_test <- predict(meta_model_rf, newdata = stacked_predictions_new, type = "response")
meta_pred_labels <- ifelse(meta_preds_test > 0.5, 1, 0)  # Assuming 0.5 as the threshold

# Calculate metrics for the stacked ensemble model
conf_matrix_meta <- table(Predicted = meta_pred_labels, Actual = test$PotentialFraud)
accuracy_meta <- sum(diag(conf_matrix_meta)) / sum(conf_matrix_meta)
precision_meta <- conf_matrix_meta[2,2] / sum(conf_matrix_meta[2,])
recall_meta <- conf_matrix_meta[2,2] / sum(conf_matrix_meta[,2])
f1_score_meta <- 2 * precision_meta * recall_meta / (precision_meta + recall_meta)

# For AUC of the stacked ensemble model
roc_obj_meta <- roc(response = test$PotentialFraud, predictor = meta_preds_test)
auc_value_meta <- auc(roc_obj_meta)

# Print the metrics for the stacked ensemble model
print("Stacked Ensemble Model Metrics:")
print(conf_matrix_meta)
print(paste("Accuracy:", round(accuracy_meta, 4)))
print(paste("Precision:", round(precision_meta, 4)))
print(paste("Recall:", round(recall_meta, 4)))
print(paste("F1-Score:", round(f1_score_meta, 4)))
print(paste("AUC:", round(auc_value_meta, 4)))

```

Performing Feature Importance Analysis:

```{r warning=FALSE, include=FALSE}
# Extract feature importance
feature_importance_rf <- importance(meta_model_rf)
```

Create a Data Frame for Visualization:

```{r warning=FALSE, include=FALSE}
# Convert to a data frame for plotting
importance_df_rf <- data.frame(Feature = rownames(feature_importance_rf), Importance = feature_importance_rf[,1])
library(ggplot2)
ggplot(importance_df_rf, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +  # For better readability of feature names
  theme_minimal() +
  labs(title = "Feature Importance in Random Forest Meta-Model", x = "Feature", y = "Importance")
```


```{r warning=FALSE, include=FALSE}
library(xgboost)
library(ggplot2)

# Assuming model1 and model2 are your XGBoost models

# Feature importance for model1
importance_model1 <- xgb.importance(feature_names = colnames(dnew), model = model1)
# Plotting
xgb.plot.importance(importance_matrix = importance_model1, top_n = 10)  # top_n for top 10 features
```

```{r warning=FALSE, include=FALSE}

# Feature importance for model2
importance_model2 <- xgb.importance(feature_names = colnames(dnew), model = model2)
# Plotting
xgb.plot.importance(importance_matrix = importance_model2, top_n = 10)  # top_n for top 10 features
```

Improved Performance: Stacking allows you to combine the predictions of multiple models, including different variations of XGBoost models. This can often result in improved predictive performance compared to a single model.

Model Robustness: By using an ensemble of models, you can reduce the risk of overfitting, enhance model generalization, and make your predictions more robust.

Flexibility: Stacking allows you to leverage the strengths of different model configurations or algorithms, potentially capturing different patterns in the data.



> # Print the metrics for the stacked ensemble model
> print("Stacked Ensemble Model Metrics:")
[1] "Stacked Ensemble Model Metrics:"
> print(conf_matrix_meta)
         Actual
Predicted     0     1
        0 31220  6250
        1  3265 15087
> print(paste("Accuracy:", round(accuracy_meta, 4)))
[1] "Accuracy: 0.8295"
> print(paste("Precision:", round(precision_meta, 4)))
[1] "Precision: 0.8221"
> print(paste("Recall:", round(recall_meta, 4)))
[1] "Recall: 0.7071"
> print(paste("F1-Score:", round(f1_score_meta, 4)))
[1] "F1-Score: 0.7603"
> print(paste("AUC:", round(auc_value_meta, 4)))
[1] "AUC: 0.9031"
> 

> # Print the metrics for the original model
> print("Original Model Metrics:")
[1] "Original Model Metrics:"
> print(conf_matrix_orig)
         Actual
Predicted     0     1
        0 31228 12536
        1  3257  8801
> print(paste("Accuracy:", round(accuracy_orig, 4)))
[1] "Accuracy: 0.7171"
> print(paste("Precision:", round(precision_orig, 4)))
[1] "Precision: 0.7299"
> print(paste("Recall:", round(recall_orig, 4)))
[1] "Recall: 0.4125"
> print(paste("F1-Score:", round(f1_score_orig, 4)))
[1] "F1-Score: 0.5271"
> print(paste("AUC:", round(auc_value_orig, 4)))
[1] "AUC: 0.7718"


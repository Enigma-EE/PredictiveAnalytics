{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Computing AND operator neural network\n",
    "\n",
    "\n",
    "Let’s assume that we have two inputs X1 and X2 which produce an output Y. The goal of this\n",
    "task will be to implement AND operator (AND gate) that has properties as outlined in Table 1.\n",
    "\n",
    "\n",
    "Table 1. AND operator\n",
    "\n",
    "| X₁ | X₂ | Y |\n",
    "|---|---|---|\n",
    "| 0  | 0  | 0  |\n",
    "| 0  | 1  | 0  |\n",
    "| 1  | 0  | 0  |\n",
    "| 1  | 1  | 1  |\n",
    "\n",
    "As Table 1 shows, only when both X1 and X2 have a value of 1, the value of Y should be 1. In all\n",
    "other cases, Y is 0.\n",
    "\n",
    "\n",
    "\n",
    "The neural network configuration for this assignment will be as follows:\n",
    "\n",
    "- One input layer with two neurons.\n",
    "  \n",
    "- One output layer with one neuron.\n",
    "  \n",
    "- The activation function for the output layer will be Sigmoid function. As a results, this means that i) Y=0 if the output layer produces a value lower than 0.5 and ii) Y=1 if the output layer produces a value greater of equal to 0.5\n",
    "\n",
    "- Initial values for weights will be equal to 1 (w=1) for all input parameters.\n",
    "\n",
    "\n",
    "- Learning rate is 0.5 (Λ = 0.5)\n",
    "\n",
    "\n",
    "Using the perceptron learning algorithm, learn the network configuration – i.e., bias and\n",
    "weights for X1 and X2 - for the given neural network.\n",
    "\n",
    "\n",
    "What is the accuracy of your model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Implementing a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can solve AND and OR gates using a perceptron. But XOR is not possible, simply because this is not a linearly separable problem. Therefore, we will follow a tutorial provided here http://theshybulb.com/2017/09/27/xor-neural-network.html to implement a neural network in Python, with a an input layer with two parameters, one hidden layer, and one output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a sigmoid function as an activation function for the hidden layer. As we want the output to be 0 or 1, we have no other option but to use the sigmoid function again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    " return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing parameters (weights and bias) is an extremely important step. Similar to what was proposed in the example referenced above, we will also assign random values between 0 and 1 for the weights, while biases will be set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    " \"\"\"\n",
    " Argument:\n",
    " n_x -- size of the input layer\n",
    " n_h -- size of the hidden layer\n",
    " n_y -- size of the output layer\n",
    " Returns:\n",
    " parameters -- python dictionary containing your parameters:\n",
    " W1 -- weight matrix of shape (n_h, n_x)\n",
    " b1 -- bias vector of shape (n_h, 1)\n",
    " W2 -- weight matrix of shape (n_y, n_h)\n",
    " b2 -- bias vector of shape (n_y, 1)\n",
    " \"\"\"\n",
    " np.random.seed(1)\n",
    " W1 = np.random.randn(n_h, n_x)\n",
    " b1 = np.zeros((n_h, 1))\n",
    " W2 = np.random.randn(n_y, n_h)\n",
    " b2 = np.zeros((n_y, 1))\n",
    " parameters = {\"W1\" : W1, \"b1\": b1,\n",
    " \"W2\" : W2, \"b2\": b2}\n",
    " return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation should be rather straightforward, and this is something we discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, Y, parameters):\n",
    " m = X.shape[1] # number of samples\n",
    " W1 = parameters[\"W1\"] # weight matrix of shape (n_h, n_x)\n",
    " W2 = parameters[\"W2\"] # weight matrix of shape (n_y, n_h)\n",
    " b1 = parameters[\"b1\"] # bias vector of shape (n_h, 1)\n",
    " b2 = parameters[\"b2\"]  # bias vector of shape (n_y, 1)\n",
    " Z1 = np.dot(W1, X) + b1 # Z1 = W1*X + b1\n",
    " A1 = sigmoid(Z1) # A1 = sigmoid(Z1)\n",
    " Z2 = np.dot(W2, A1) + b2 # Z2 = W2*A1 + b2\n",
    " A2 = sigmoid(Z2) # A2 = sigmoid(Z2)\n",
    " cache = (Z1, A1, W1, b1, Z2, A2, W2, b2) # cache for backpropagation\n",
    " logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), (1\n",
    "- Y)) # compute cost\n",
    " cost = -np.sum(logprobs) / m # compute cost\n",
    " return cost, cache, A2 # return cost, cache and A2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to implement the backpropagation part, as we have one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache): # backward propagation\n",
    " m = X.shape[1] # initialize number of samples, m is number of samples\n",
    " (Z1, A1, W1, b1, Z2, A2, W2, b2) = cache # unpack cache, Contains forward propagation intermediate values, useful for backward computation\n",
    " # Second Layer Gradient Computations (Output Layer):\n",
    " dZ2 = A2 - Y  # This is the gradient of the loss with respect to Z2. It represents how much the output (A2) differs from the actual value (Y). In simple terms, it's the error of the output\n",
    " dW2 = np.dot(dZ2, A1.T) / m  # Gradient of the loss with respect to W2. It indicates how much we need to adjust W2 to minimize the error\n",
    " db2 = np.sum(dZ2, axis = 1, keepdims=True) / m #  Gradient of the loss with respect to b2. It indicates how much we need to adjust b2 to minimize the error\n",
    "\n",
    "# First Layer Gradient Computations:\n",
    " dA1 = np.dot(W2.T, dZ2)  # Gradient of the loss with respect to A1.\n",
    " dZ1 = np.multiply(dA1, A1 * (1- A1)) #  Gradient of the loss with respect to Z1. The term A1 * (1- A1) implies that the activation function used in layer 1 is the sigmoid \n",
    " dW1 = np.dot(dZ1, X.T) / m # Gradient of the loss with respect to W1. It indicates how much we need to adjust W1 to minimize the error\n",
    " db1 = np.sum(dZ1, axis=1, keepdims=True) / m # Gradient of the loss with respect to b1. It indicates how much we need to adjust b1 to minimize the error\n",
    "\n",
    "\n",
    "# Store Gradients\n",
    " gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    " \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    " return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    " parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n",
    " parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n",
    " parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n",
    " parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n",
    " return parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to learn the XOR table and evaluate the performance of our model, we would have\n",
    "something along those lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation:\n",
    "\n",
    "Note: I'm assuming sigmoid activation for both layers; you can change as per your original design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, Y, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
    "    \n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "    cost = -np.sum(logprobs) / Y.shape[1]\n",
    "\n",
    "    return cost, cache, A2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * grads[\"dW1\"]\n",
    "    parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * grads[\"dW2\"]\n",
    "    parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * grads[\"db1\"]\n",
    "    parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * grads[\"db2\"]\n",
    "    \n",
    "    return parameters  # Corrected this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999564 0.4999913  0.50000868 0.50000434]]\n",
      "[[0. 0. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]]) # XOR\n",
    "n_h = 2\n",
    "n_x = X.shape[0]\n",
    "n_y = Y.shape[0]\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "num_iterations = 100000\n",
    "learning_rate = 0.01\n",
    "losses = np.zeros((num_iterations, 1))\n",
    "for i in range(num_iterations):\n",
    " losses[i, 0], cache, A2 = forward_propagation(X, Y, parameters)\n",
    " grads = backward_propagation(X, Y, cache)\n",
    " parameters = update_parameters(parameters, grads, learning_rate)\n",
    "cost, _, A2 = forward_propagation(X, Y, parameters)\n",
    "pred = (A2 > 0.5) * 1.0\n",
    "print(A2)\n",
    "print(pred)\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use these functions with the code you provided to train the model and visualize the losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key Observations**:\n",
    "\n",
    "1. **Sharp Decrease at the Beginning**: The loss starts at a high value and experiences a sharp decline in the initial iterations. This indicates that the model is rapidly learning to fit the data in the beginning stages of the training.\n",
    "\n",
    "2. **Plateau after the Decrease**: After the rapid decline, the loss value seems to stabilize and doesn't change much. This plateau suggests that the model has reached a point where further training doesn't significantly reduce the loss value. It might have found an optimal or near-optimal set of parameters (weights and biases) for the given learning rate and architecture.\n",
    "\n",
    "**Interpretation**:\n",
    "\n",
    "1. **Model's Learning**: The sharp initial decrease indicates the model's ability to learn from the data. As the model updates its parameters (weights and biases), it becomes better at its task, leading to a reduction in the error (loss).\n",
    "\n",
    "2. **Convergence**: The plateauing of the loss suggests that the model has converged, meaning it has reached a point where further training doesn't lead to significant improvements. This can be due to various reasons:\n",
    "   - The model has reached its capacity in learning the given data with the current architecture.\n",
    "   - The learning rate might be too high for fine-tuning, causing it to oscillate around a minimum value without significantly reducing the loss.\n",
    "   - The model might have reached a local minimum in the loss surface.\n",
    "\n",
    "**Recommendations**:\n",
    "\n",
    "1. **Evaluation**: It's essential to evaluate the model on a separate validation or test set to ensure that it's not just memorizing the training data (overfitting). If the model performs well on unseen data, it indicates good generalization.\n",
    "\n",
    "2. **Hyperparameter Tuning**: If you're not satisfied with the performance, consider experimenting with hyperparameters like the learning rate, the number of hidden units, or the activation functions. \n",
    "\n",
    "3. **Early Stopping**: Since the loss stabilizes after a certain number of iterations, you could employ early stopping. This means you can halt the training once the loss stops decreasing (or decreases very slowly) to save computational resources and time.\n",
    "\n",
    "In summary, this graph shows the typical behavior of a model that's learning and then converging. The next steps would involve evaluating its predictions on unseen data and potentially tuning it further for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
